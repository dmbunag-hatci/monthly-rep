{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a32869da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IMPORTANT CHECK:\n",
      "This script is intended to run on workbooks where all formulas have been\n",
      "replaced by VALUES (copy + paste by values) to avoid breaking references.\n",
      "\n",
      "\n",
      "Please enter the folder path containing the 3 workbooks (files whose names contain '3M', '12WM', and '36WM').\n",
      "\n",
      "Using input folder: C:\\Users\\r103317\\Projects\\Monthly Report\\monthly-reporter\\upload\n",
      "Outputs will be written to: C:\\Users\\r103317\\Projects\\Monthly Report\\monthly-reporter\\outputs\n",
      "\n",
      "Searching for workbooks in: C:\\Users\\r103317\\Projects\\Monthly Report\\monthly-reporter\\upload\n",
      "Found the following .xlsx files:\n",
      "  - 26.1-12WM_update.xlsx\n",
      "  - 26.1-36WM_update.xlsx\n",
      "  - 26.1-3M_update.xlsx\n",
      "\n",
      "Tag -> matched files:\n",
      "  For tag '3M': OK -> 26.1-3M_update.xlsx\n",
      "  For tag '12WM': OK -> 26.1-12WM_update.xlsx\n",
      "  For tag '36WM': OK -> 26.1-36WM_update.xlsx\n",
      "\n",
      "Workbook mapping (final):\n",
      "  3M -> 26.1-3M_update.xlsx\n",
      "  12WM -> 26.1-12WM_update.xlsx\n",
      "  36WM -> 26.1-36WM_update.xlsx\n",
      "\n",
      "⏳ Processing 3M workbook: 26.1-3M_update.xlsx\n",
      "✅ Finished processing 3M.\n",
      "  Output workbook: 3M-full_update-translated.xlsx\n",
      "  Scanned text cells (non-formula): 8518\n",
      "  Changed cells: 4497\n",
      "\n",
      "⏳ Processing 12WM workbook: 26.1-12WM_update.xlsx\n",
      "✅ Finished processing 12WM.\n",
      "  Output workbook: 12WM-full_update-translated.xlsx\n",
      "  Scanned text cells (non-formula): 103661\n",
      "  Changed cells: 14054\n",
      "\n",
      "⏳ Processing 36WM workbook: 26.1-36WM_update.xlsx\n",
      "✅ Finished processing 36WM.\n",
      "  Output workbook: 36WM-full_update-translated.xlsx\n",
      "  Scanned text cells (non-formula): 131894\n",
      "  Changed cells: 26012\n",
      "\n",
      "=== All workbooks processed ===\n",
      "Total scanned text cells (non-formula): 244073\n",
      "Total changed cells: 44563\n",
      "Missing unique Hangul items across all workbooks: 0 -> missing_items.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Translate Excel SHEET CONTENTS (cell text) using a user-provided map.json.\n",
    "\n",
    "- Asks user:\n",
    "  1) Did you copy + paste by values in all sheets within all workbooks?\n",
    "  2) Folder path containing the 3M / 12WM / 36WM workbooks.\n",
    "\n",
    "- Does NOT rename existing sheets\n",
    "- Adds a new first sheet \"Sheet Name Trans.\" with:\n",
    "  * Original sheet names\n",
    "  * English translations of sheet names (using map.json)\n",
    "- \"Sheet Name Trans.\" is NOT scanned/translated like other sheets.\n",
    "\n",
    "- Does NOT modify formulas (skips them by default)\n",
    "- Does NOT overwrite map.json (read-only)\n",
    "- Uses strict Hangul-with-spaces phrases + remaining Hangul runs\n",
    "- Writes:\n",
    "  * 3M-full_update-translated.xlsx\n",
    "  * 12WM-full_update-translated.xlsx\n",
    "  * 36WM-full_update-translated.xlsx\n",
    "  * missing_items.csv (Hangul detected but not present in map.json)\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import json\n",
    "import collections\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "\n",
    "# Tags we expect in filenames to identify each workbook\n",
    "WORKBOOK_TAGS = [\"3M\", \"12WM\", \"36WM\"]\n",
    "\n",
    "# Use your cleaned map file here (or rename it to map.json)\n",
    "MAP_JSON = \"map.json\"\n",
    "\n",
    "# Name for the missing items CSV (will be written into outputs/ folder)\n",
    "MISSING_CSV = \"missing_items.csv\"\n",
    "\n",
    "SKIP_FORMULAS = True\n",
    "APPLY_SPACE_NORMALIZATION = True\n",
    "\n",
    "SHEET_NAME_TRANSLATION_TITLE = \"Sheet Name Trans.\"\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# HANGUL DETECTION / EXTRACTION\n",
    "# -----------------------------\n",
    "HANGUL_CHAR_RE = re.compile(r\"[\\uac00-\\ud7a3]\")\n",
    "HANGUL_PHRASE_RE = re.compile(r\"[\\uac00-\\ud7a3]+(?:\\s+[\\uac00-\\ud7a3]+)+\")\n",
    "HANGUL_RUN_RE = re.compile(r\"[\\uac00-\\ud7a3]+\")\n",
    "\n",
    "\n",
    "def has_hangul(s: str) -> bool:\n",
    "    return isinstance(s, str) and bool(HANGUL_CHAR_RE.search(s))\n",
    "\n",
    "\n",
    "def extract_phrases_and_runs_strict(text: str) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Extract:\n",
    "      - phrases: Hangul words separated by spaces\n",
    "      - runs: remaining contiguous Hangul sequences after removing phrases\n",
    "    \"\"\"\n",
    "    phrases = HANGUL_PHRASE_RE.findall(text)\n",
    "    remainder = text\n",
    "    for p in phrases:\n",
    "        remainder = remainder.replace(p, \" \")\n",
    "    runs = HANGUL_RUN_RE.findall(remainder)\n",
    "    return phrases, runs\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# OPTIONAL SPACE NORMALIZATION\n",
    "# -----------------------------\n",
    "def normalize_spaces(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # ) followed by alnum: add space\n",
    "    text = re.sub(r\"\\)(?=[A-Za-z0-9])\", \") \", text)\n",
    "    # Alpha right next to Hangul (both directions): add space\n",
    "    text = re.sub(r\"([A-Za-z])([\\uac00-\\ud7a3])\", r\"\\1 \\2\", text)\n",
    "    text = re.sub(r\"([\\uac00-\\ud7a3])([A-Za-z])\", r\"\\1 \\2\", text)\n",
    "    # Hangul right before \"Plant\" (with optional digits): add space\n",
    "    text = re.sub(r\"([\\uac00-\\ud7a3])(\\d*Plant)\", r\"\\1 \\2\", text)\n",
    "    # Collapse multiple spaces\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# TOKEN PROTECTION\n",
    "# -----------------------------\n",
    "# Protect codes/ids/numbers; protect parentheses only if NO Hangul inside.\n",
    "TOKEN_PATTERNS = [\n",
    "    r\"\\[[^\\]]+\\]\",                           # [ABC123]\n",
    "    r\"\\b[A-Z]{1,5}\\d{0,5}\\b\",                # codes like ABC12\n",
    "    r\"\\b\\d+(?:\\.\\d+)?(?:E[-+]\\d+)?\\b\",       # numbers, floats, scientific notation\n",
    "    r\"\\((?!.*[\\uac00-\\ud7a3])[^\\)]*\\)\",      # (...) with no Hangul inside\n",
    "]\n",
    "TOKEN_RE = re.compile(\"|\".join(f\"({p})\" for p in TOKEN_PATTERNS))\n",
    "\n",
    "\n",
    "def protect_tokens(text: str) -> Tuple[str, List[str]]:\n",
    "    tokens: List[str] = []\n",
    "\n",
    "    def repl(m):\n",
    "        tokens.append(m.group(0))\n",
    "        return f\"__TOK{len(tokens)-1}__\"\n",
    "\n",
    "    return TOKEN_RE.sub(repl, text), tokens\n",
    "\n",
    "\n",
    "def restore_tokens(text: str, tokens: List[str]) -> str:\n",
    "    for i, tok in enumerate(tokens):\n",
    "        text = text.replace(f\"__TOK{i}__\", tok)\n",
    "    return text\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# MAP LOADING (READ-ONLY)\n",
    "# -----------------------------\n",
    "def load_fragments_map(path: str) -> Dict[str, str]:\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Map file not found: {p.resolve()}\")\n",
    "    with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    fragments = data.get(\"fragments\", {})\n",
    "    if not isinstance(fragments, dict):\n",
    "        raise ValueError('Map must look like: {\"fragments\": { ... }}')\n",
    "    return fragments\n",
    "\n",
    "\n",
    "def build_pairs(fragments: Dict[str, str]) -> List[Tuple[str, str]]:\n",
    "    # Sort keys longest-first to avoid partial-replacement collisions\n",
    "    keys = sorted(fragments.keys(), key=len, reverse=True)\n",
    "    return [(k, fragments[k]) for k in keys]\n",
    "\n",
    "\n",
    "def translate_text_with_map(text: str, pairs: List[Tuple[str, str]]) -> str:\n",
    "    masked, toks = protect_tokens(text)\n",
    "    out = masked\n",
    "    for k, v in pairs:\n",
    "        out = out.replace(k, v)\n",
    "    out = restore_tokens(out, toks)\n",
    "    return out\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# WORKBOOK DISCOVERY\n",
    "# -----------------------------\n",
    "def find_workbooks_by_tag(folder: Path, tags: List[str]) -> Optional[Dict[str, Path]]:\n",
    "    \"\"\"\n",
    "    Scan folder for .xlsx files and map each tag (e.g. '3M') to exactly one file\n",
    "    whose name contains that tag.\n",
    "\n",
    "    Returns:\n",
    "        dict[tag -> Path] if successful, otherwise None (and prints errors).\n",
    "    \"\"\"\n",
    "    if not folder.is_dir():\n",
    "        print(f\"\\nERROR: Input folder not found: {folder.resolve()}\")\n",
    "        return None\n",
    "\n",
    "    all_xlsx = [p for p in folder.glob(\"*.xlsx\") if not p.name.startswith(\"~$\")]\n",
    "\n",
    "    print(\"\\nSearching for workbooks in:\", folder.resolve())\n",
    "    if not all_xlsx:\n",
    "        print(\"No .xlsx files found in this folder.\")\n",
    "    else:\n",
    "        print(\"Found the following .xlsx files:\")\n",
    "        for p in all_xlsx:\n",
    "            print(f\"  - {p.name}\")\n",
    "\n",
    "    mapping: Dict[str, Path] = {}\n",
    "    errors: List[str] = []\n",
    "\n",
    "    print(\"\\nTag -> matched files:\")\n",
    "    for tag in tags:\n",
    "        matches = [p for p in all_xlsx if tag in p.name]\n",
    "\n",
    "        if len(matches) == 0:\n",
    "            msg = (\n",
    "                f\"  For tag '{tag}': expected 1 file, found 0. \"\n",
    "                f\"No filenames containing '{tag}'.\"\n",
    "            )\n",
    "            print(msg)\n",
    "            errors.append(msg)\n",
    "        elif len(matches) == 1:\n",
    "            print(f\"  For tag '{tag}': OK -> {matches[0].name}\")\n",
    "            mapping[tag] = matches[0]\n",
    "        else:\n",
    "            names = \", \".join(m.name for m in matches)\n",
    "            msg = (\n",
    "                f\"  For tag '{tag}': expected 1 file, found {len(matches)}: {names}\"\n",
    "            )\n",
    "            print(msg)\n",
    "            errors.append(msg)\n",
    "\n",
    "    if errors:\n",
    "        print(\"\\nERROR: Workbook detection failed.\")\n",
    "        print(\"Details:\")\n",
    "        for e in errors:\n",
    "            print(\" -\", e)\n",
    "        print(\n",
    "            \"\\nPlease ensure there is exactly one file containing each of \"\n",
    "            f\"{', '.join(tags)} in the selected folder, then re-run the script.\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    print(\"\\nWorkbook mapping (final):\")\n",
    "    for tag, path in mapping.items():\n",
    "        print(f\"  {tag} -> {path.name}\")\n",
    "\n",
    "    return mapping\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# SHEET NAME TRANSLATION SHEET\n",
    "# -----------------------------\n",
    "def add_sheet_name_translation_sheet(\n",
    "    wb,\n",
    "    input_path: Path,\n",
    "    fragments: Dict[str, str],\n",
    "    pairs: List[Tuple[str, str]],\n",
    "    missing_counts: collections.Counter,\n",
    "    missing_example: Dict[str, str],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Create a new first sheet \"Sheet Name Trans.\" listing:\n",
    "      - Original sheet names\n",
    "      - Translated sheet names\n",
    "\n",
    "    NOTE: This version does NOT contribute sheet-name Hangul\n",
    "    to missing_counts / missing_example. missing_items.csv\n",
    "    now reflects ONLY cell contents.\n",
    "    \"\"\"\n",
    "    # Get original sheet names (before adding the new sheet)\n",
    "    original_sheet_names = list(wb.sheetnames)\n",
    "\n",
    "    # Compute translations for sheet names (no missing-items tracking here)\n",
    "    translations: List[Tuple[str, str]] = []\n",
    "    for sheet_name in original_sheet_names:\n",
    "        original = sheet_name\n",
    "        processed = original.strip()\n",
    "        if APPLY_SPACE_NORMALIZATION:\n",
    "            processed = normalize_spaces(processed)\n",
    "\n",
    "        # Translate sheet name (for display only; we do NOT rename sheets)\n",
    "        translated = translate_text_with_map(processed, pairs)\n",
    "        if APPLY_SPACE_NORMALIZATION:\n",
    "            translated = normalize_spaces(translated)\n",
    "\n",
    "        translations.append((original, translated))\n",
    "\n",
    "    # Create the new sheet at position 0\n",
    "    sheet = wb.create_sheet(title=SHEET_NAME_TRANSLATION_TITLE, index=0)\n",
    "\n",
    "    # Header row\n",
    "    sheet[\"A1\"] = \"Original Sheet Name\"\n",
    "    sheet[\"B1\"] = \"Translated Sheet Name\"\n",
    "\n",
    "    # Data rows\n",
    "    row_idx = 2\n",
    "    for original, translated in translations:\n",
    "        sheet.cell(row=row_idx, column=1, value=original)\n",
    "        sheet.cell(row=row_idx, column=2, value=translated)\n",
    "        row_idx += 1\n",
    "# -----------------------------\n",
    "# MAIN\n",
    "# -----------------------------\n",
    "def main():\n",
    "    # 1. Safety confirmation in terminal\n",
    "    print(\n",
    "        \"\\nIMPORTANT CHECK:\\n\"\n",
    "        \"This script is intended to run on workbooks where all formulas have been\\n\"\n",
    "        \"replaced by VALUES (copy + paste by values) to avoid breaking references.\\n\"\n",
    "    )\n",
    "    answer = input(\n",
    "        \"Did you copy + paste by values in all sheets within all workbooks? [y/N]: \"\n",
    "    ).strip().lower()\n",
    "\n",
    "    if answer not in (\"y\", \"yes\"):\n",
    "        print(\n",
    "            \"\\nAborting: Please create value-only versions of your workbooks first, \"\n",
    "            \"then re-run this script.\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    # 2. Ask user for folder path\n",
    "    print(\n",
    "        \"\\nPlease enter the folder path containing the 3 workbooks \"\n",
    "        \"(files whose names contain '3M', '12WM', and '36WM').\"\n",
    "    )\n",
    "    folder_input = input(\"Folder path: \").strip()\n",
    "\n",
    "    if not folder_input:\n",
    "        print(\"\\nNo folder path entered. Aborting.\")\n",
    "        return\n",
    "\n",
    "    # Allow user to paste with quotes; strip leading/trailing quotes if present\n",
    "    folder_input = folder_input.strip().strip('\"').strip(\"'\")\n",
    "    input_folder = Path(folder_input)\n",
    "    print(f\"\\nUsing input folder: {input_folder.resolve()}\")\n",
    "\n",
    "    # 3. Resolve script directory and outputs folder\n",
    "    try:\n",
    "        script_dir = Path(__file__).resolve().parent\n",
    "    except NameError:\n",
    "        # Fallback if __file__ is not defined (e.g. in some interactive environments)\n",
    "        script_dir = Path.cwd()\n",
    "\n",
    "    outputs_dir = script_dir / \"outputs\"\n",
    "    outputs_dir.mkdir(exist_ok=True)\n",
    "    print(f\"Outputs will be written to: {outputs_dir.resolve()}\")\n",
    "\n",
    "    # 4. Load translation map (fragments)\n",
    "    fragments = load_fragments_map(MAP_JSON)\n",
    "    pairs = build_pairs(fragments)\n",
    "\n",
    "    # 5. Discover the 3 target workbooks in the input folder\n",
    "    files_by_tag = find_workbooks_by_tag(input_folder, WORKBOOK_TAGS)\n",
    "    if files_by_tag is None:\n",
    "        # Errors already printed; just abort\n",
    "        return\n",
    "\n",
    "    # 6. Global counters for missing items across ALL workbooks\n",
    "    missing_counts = collections.Counter()\n",
    "    missing_example: Dict[str, str] = {}\n",
    "\n",
    "    total_scanned = 0\n",
    "    total_changed = 0\n",
    "\n",
    "    # 7. Process each workbook\n",
    "    for tag, input_path in files_by_tag.items():\n",
    "        print(f\"\\n⏳ Processing {tag} workbook: {input_path.name}\")\n",
    "\n",
    "        wb = load_workbook(input_path, data_only=False)\n",
    "\n",
    "        # 7a. Add \"Sheet Name Trans.\" sheet with sheet-name translations\n",
    "        add_sheet_name_translation_sheet(\n",
    "            wb, input_path, fragments, pairs, missing_counts, missing_example\n",
    "        )\n",
    "\n",
    "        scanned = 0\n",
    "        changed = 0\n",
    "\n",
    "        # 7b. Translate cell contents on all sheets EXCEPT \"Sheet Name Trans.\"\n",
    "        for ws in wb.worksheets:\n",
    "            if ws.title == SHEET_NAME_TRANSLATION_TITLE:\n",
    "                # Skip the translation sheet itself\n",
    "                continue\n",
    "\n",
    "            for row in ws.iter_rows():\n",
    "                for cell in row:\n",
    "                    v = cell.value\n",
    "                    if not isinstance(v, str):\n",
    "                        continue\n",
    "                    if SKIP_FORMULAS and v.startswith(\"=\"):\n",
    "                        continue\n",
    "\n",
    "                    scanned += 1\n",
    "                    text = v.strip()\n",
    "                    if APPLY_SPACE_NORMALIZATION:\n",
    "                        text = normalize_spaces(text)\n",
    "\n",
    "                    if not has_hangul(text):\n",
    "                        # Still write normalized text (e.g. spacing changes) if needed\n",
    "                        if APPLY_SPACE_NORMALIZATION and text != v:\n",
    "                            cell.value = text\n",
    "                        continue\n",
    "\n",
    "                    # Missing Hangul detection for cell contents\n",
    "                    phrases, runs = extract_phrases_and_runs_strict(text)\n",
    "                    for item in phrases + runs:\n",
    "                        if item and item not in fragments:\n",
    "                            missing_counts[item] += 1\n",
    "                            example_key = (\n",
    "                                f\"{input_path.name}:{ws.title}!{cell.coordinate}\"\n",
    "                            )\n",
    "                            missing_example.setdefault(item, example_key)\n",
    "\n",
    "                    # Translation\n",
    "                    out = translate_text_with_map(text, pairs)\n",
    "                    if APPLY_SPACE_NORMALIZATION:\n",
    "                        out = normalize_spaces(out)\n",
    "\n",
    "                    if out != v:\n",
    "                        cell.value = out\n",
    "                        changed += 1\n",
    "\n",
    "        # 7c. Save translated workbook for this tag\n",
    "        output_name = f\"{tag}-full_update-translated.xlsx\"\n",
    "        output_path = outputs_dir / output_name\n",
    "        wb.save(output_path)\n",
    "\n",
    "        print(f\"✅ Finished processing {tag}.\")\n",
    "        print(f\"  Output workbook: {output_path.name}\")\n",
    "        print(f\"  Scanned text cells (non-formula): {scanned}\")\n",
    "        print(f\"  Changed cells: {changed}\")\n",
    "\n",
    "        total_scanned += scanned\n",
    "        total_changed += changed\n",
    "\n",
    "    # 8. Write combined missing_items.csv for all workbooks\n",
    "    df_missing = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"hangul\": k,\n",
    "                \"count\": int(c),\n",
    "                \"example_cell\": missing_example.get(k, \"\"),\n",
    "            }\n",
    "            for k, c in missing_counts.most_common()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    missing_csv_path = outputs_dir / MISSING_CSV\n",
    "    df_missing.to_csv(missing_csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(\"\\n=== All workbooks processed ===\")\n",
    "    print(f\"Total scanned text cells (non-formula): {total_scanned}\")\n",
    "    print(f\"Total changed cells: {total_changed}\")\n",
    "    print(\n",
    "        f\"Missing unique Hangul items across all workbooks: {len(missing_counts)} \"\n",
    "        f\"-> {missing_csv_path.name}\"\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monthly-reporter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
