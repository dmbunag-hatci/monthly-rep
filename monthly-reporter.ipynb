{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a32869da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Output workbook: 2025.12-36WM-translated.xlsx\n",
      "Scanned text cells (non-formula): 43896\n",
      "Changed cells: 31970\n",
      "Missing unique Hangul items: 0 -> missing_items.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Translate Excel SHEET CONTENTS (cell text) using a user-provided map.json.\n",
    "\n",
    "- Does NOT rename sheets\n",
    "- Does NOT modify formulas (skips them by default)\n",
    "- Does NOT overwrite map.json (read-only)\n",
    "- Uses strict Hangul-with-spaces phrases (Option A) + remaining Hangul runs\n",
    "- Writes:\n",
    "  * translated workbook\n",
    "  * missing_items.csv (Hangul detected but not present in map.json)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import json\n",
    "import collections\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "INPUT_XLSX  = r\"C:\\Users\\r103317\\Projects\\Monthly Report\\.venv\\25.12-36WM_full.xlsx\"\n",
    "OUTPUT_XLSX = \"2025.12-36WM-translated.xlsx\"\n",
    "\n",
    "# Use your cleaned map file here (or rename it to map.json)\n",
    "MAP_JSON = \"map.json\"\n",
    "\n",
    "MISSING_CSV  = \"missing_items.csv\"\n",
    "\n",
    "SKIP_FORMULAS = True\n",
    "APPLY_SPACE_NORMALIZATION = True\n",
    "\n",
    "# -----------------------------\n",
    "# HANGUL DETECTION / EXTRACTION\n",
    "# -----------------------------\n",
    "HANGUL_CHAR_RE = re.compile(r\"[\\uac00-\\ud7a3]\")\n",
    "HANGUL_PHRASE_RE = re.compile(r\"[\\uac00-\\ud7a3]+(?:\\s+[\\uac00-\\ud7a3]+)+\")\n",
    "HANGUL_RUN_RE = re.compile(r\"[\\uac00-\\ud7a3]+\")\n",
    "\n",
    "def has_hangul(s: str) -> bool:\n",
    "    return isinstance(s, str) and bool(HANGUL_CHAR_RE.search(s))\n",
    "\n",
    "def extract_phrases_and_runs_strict(text: str) -> Tuple[List[str], List[str]]:\n",
    "    phrases = HANGUL_PHRASE_RE.findall(text)\n",
    "    remainder = text\n",
    "    for p in phrases:\n",
    "        remainder = remainder.replace(p, \" \")\n",
    "    runs = HANGUL_RUN_RE.findall(remainder)\n",
    "    return phrases, runs\n",
    "\n",
    "# -----------------------------\n",
    "# OPTIONAL SPACE NORMALIZATION\n",
    "# -----------------------------\n",
    "def normalize_spaces(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    text = re.sub(r\"\\)(?=[A-Za-z0-9])\", \") \", text)\n",
    "    text = re.sub(r\"([A-Za-z])([\\uac00-\\ud7a3])\", r\"\\1 \\2\", text)\n",
    "    text = re.sub(r\"([\\uac00-\\ud7a3])([A-Za-z])\", r\"\\1 \\2\", text)\n",
    "    text = re.sub(r\"([\\uac00-\\ud7a3])(\\d*Plant)\", r\"\\1 \\2\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# -----------------------------\n",
    "# TOKEN PROTECTION\n",
    "# -----------------------------\n",
    "# Protect codes/ids/numbers; protect parentheses only if NO Hangul inside.\n",
    "TOKEN_PATTERNS = [\n",
    "    r\"\\[[^\\]]+\\]\",\n",
    "    r\"\\b[A-Z]{1,5}\\d{0,5}\\b\",\n",
    "    r\"\\b\\d+(?:\\.\\d+)?(?:E[-+]\\d+)?\\b\",\n",
    "    r\"\\((?!.*[\\uac00-\\ud7a3])[^\\)]*\\)\",\n",
    "]\n",
    "TOKEN_RE = re.compile(\"|\".join(f\"({p})\" for p in TOKEN_PATTERNS))\n",
    "\n",
    "def protect_tokens(text: str) -> Tuple[str, List[str]]:\n",
    "    tokens: List[str] = []\n",
    "    def repl(m):\n",
    "        tokens.append(m.group(0))\n",
    "        return f\"__TOK{len(tokens)-1}__\"\n",
    "    return TOKEN_RE.sub(repl, text), tokens\n",
    "\n",
    "def restore_tokens(text: str, tokens: List[str]) -> str:\n",
    "    for i, tok in enumerate(tokens):\n",
    "        text = text.replace(f\"__TOK{i}__\", tok)\n",
    "    return text\n",
    "\n",
    "# -----------------------------\n",
    "# MAP LOADING (READ-ONLY)\n",
    "# -----------------------------\n",
    "def load_fragments_map(path: str) -> Dict[str, str]:\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Map file not found: {p.resolve()}\")\n",
    "    with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    fragments = data.get(\"fragments\", {})\n",
    "    if not isinstance(fragments, dict):\n",
    "        raise ValueError(\"Map must look like: {\\\"fragments\\\": { ... }}\")\n",
    "    return fragments\n",
    "\n",
    "def build_pairs(fragments: Dict[str, str]) -> List[Tuple[str, str]]:\n",
    "    keys = sorted(fragments.keys(), key=len, reverse=True)\n",
    "    return [(k, fragments[k]) for k in keys]\n",
    "\n",
    "def translate_text_with_map(text: str, pairs: List[Tuple[str, str]]) -> str:\n",
    "    masked, toks = protect_tokens(text)\n",
    "    out = masked\n",
    "    for k, v in pairs:\n",
    "        out = out.replace(k, v)\n",
    "    out = restore_tokens(out, toks)\n",
    "    return out\n",
    "\n",
    "# -----------------------------\n",
    "# MAIN\n",
    "# -----------------------------\n",
    "def main():\n",
    "    fragments = load_fragments_map(MAP_JSON)\n",
    "    pairs = build_pairs(fragments)\n",
    "\n",
    "    wb = load_workbook(INPUT_XLSX, data_only=False)\n",
    "\n",
    "    missing_counts = collections.Counter()\n",
    "    missing_example = {}\n",
    "\n",
    "    scanned = 0\n",
    "    changed = 0\n",
    "\n",
    "    for ws in wb.worksheets:\n",
    "        for row in ws.iter_rows():\n",
    "            for cell in row:\n",
    "                v = cell.value\n",
    "                if not isinstance(v, str):\n",
    "                    continue\n",
    "                if SKIP_FORMULAS and v.startswith(\"=\"):\n",
    "                    continue\n",
    "\n",
    "                scanned += 1\n",
    "                text = v.strip()\n",
    "                if APPLY_SPACE_NORMALIZATION:\n",
    "                    text = normalize_spaces(text)\n",
    "\n",
    "                if not has_hangul(text):\n",
    "                    if APPLY_SPACE_NORMALIZATION and text != v:\n",
    "                        cell.value = text\n",
    "                    continue\n",
    "\n",
    "                # report missing (if any)\n",
    "                phrases, runs = extract_phrases_and_runs_strict(text)\n",
    "                for item in phrases + runs:\n",
    "                    if item and item not in fragments:\n",
    "                        missing_counts[item] += 1\n",
    "                        missing_example.setdefault(item, f\"{ws.title}!{cell.coordinate}\")\n",
    "\n",
    "                out = translate_text_with_map(text, pairs)\n",
    "                if APPLY_SPACE_NORMALIZATION:\n",
    "                    out = normalize_spaces(out)\n",
    "\n",
    "                if out != v:\n",
    "                    cell.value = out\n",
    "                    changed += 1\n",
    "\n",
    "    wb.save(OUTPUT_XLSX)\n",
    "\n",
    "    df_missing = pd.DataFrame(\n",
    "        [{\"hangul\": k, \"count\": int(c), \"example_cell\": missing_example.get(k, \"\")}\n",
    "         for k, c in missing_counts.most_common()]\n",
    "    )\n",
    "    df_missing.to_csv(MISSING_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(\"Done.\")\n",
    "    print(f\"Output workbook: {OUTPUT_XLSX}\")\n",
    "    print(f\"Scanned text cells (non-formula): {scanned}\")\n",
    "    print(f\"Changed cells: {changed}\")\n",
    "    print(f\"Missing unique Hangul items: {len(missing_counts)} -> {MISSING_CSV}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Monthly Report",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
